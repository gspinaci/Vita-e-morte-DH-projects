{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gspinaci/Vita-e-morte-DH-projects/blob/main/giorgia_crawlerDHprojects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c7NPLAnWPtxb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL to the spreadsheet file (e.g., Google Sheets in .xlsx format)\n",
    "url = \"https://docs.google.com/spreadsheets/d/1G3WiRMoopP8Y2FlVJfwCRqZVLilw-aMQRBxF_SoMF20/edit?gid=647909856#gid=647909856\"\n",
    "\n",
    "# Extract the spreadsheet ID from the URL\n",
    "spreadsheet_id = url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "\n",
    "# Construct the download URL for Google Sheets\n",
    "download_url = f\"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=xlsx\"\n",
    "\n",
    "# Load the spreadsheet using the download URL and specifying the engine\n",
    "df = pd.read_excel(download_url, sheet_name=\"Centri DH\", engine=\"openpyxl\")\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CPuusYWMGzf",
    "outputId": "875dbedd-2de0-42b2-f7f1-eb6dfe60bea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Digital Humanities Advanced Research Centre (DH.ARC) - Unibo...\n",
      "  Processing URL: https://centri.unibo.it/dharc/en/research/projects-at-dh-arc\n",
      "Processing Venice Centre for Digital and Public Humanities (VeDPH)...\n",
      "  Processing URL: https://www.unive.it/pag/47701/\n",
      "  Processing URL: https://www.unive.it/pag/47702/\n",
      "  Processing URL: https://www.unive.it/pag/47703/\n",
      "  Processing URL: https://www.unive.it/pag/47704/\n",
      "Processing Digital Culture Laboratory - Pisa...\n",
      "  Processing URL: https://www.labcd.unipi.it/progetti/\n",
      "Processing DigiLab – Interdepartmental Center for research and services - la sapienza...\n",
      "  Processing URL: https://digilab.uniroma1.it/ricerca/progetti-corso\n",
      "Processing Interdepartmental center FiTMU – DH Section - unisalerno...\n",
      "Processing Laboratory Vast-Lab - Prato ...\n",
      "  Processing URL: https://vast-lab.org/progetti/\n",
      "Processing Centro interdipartimentale di ricerca in Digital Humanities - Università del Salento...\n",
      "Processing CRR-MM - unibo...\n",
      "  Processing URL: https://sba.unibo.it/it/almadl/collezioni/portfolio-crrmm\n",
      "Processing Centro Interdipartimentale di Ricerca \"Digital Scholarship for the Humanities\" DISH Università di Torino...\n",
      "Processing Centro Interdisciplinare di Ricerche per la Computerizzazione dei Segni dell'Espressione Università Cattolica di Milano...\n",
      "  Processing URL: https://centridiricerca.unicatt.it/circse/it/progetti.html\n",
      "Processing Centro di Informatica Umanistica - Università degli Studi di Catania...\n",
      "Processing Laboratorio CRILeT \"Giuseppe Gigliozzi\" - Sapienza Università di Roma...\n",
      "Processing Laboratorio di Informatica Umanistica e Cultura Digitale - Università di Parma...\n",
      "  Processing URL: https://dhlab.unipr.it/?page_id=178\n",
      "Processing Digital Arena for Inclusive Humanities - Università di Verona...\n",
      "Processing LUDICA: Laboratorio di Umanistica Digitale Università di Cagliari...\n",
      "  Processing URL: https://www.unica.it/unica/it/news_avvisi_s1.page?contentId=AVS162074\n",
      "Processing nan...\n",
      "Processing Digital humanities lab - kunsthistoriches florenz...\n",
      "  Processing URL: https://www.khi.fi.it/en/forschung/digital-humanities/index.php\n",
      "Processing Digitatti - Villa I tatti Florence...\n",
      "  Processing URL: https://florentinedrawings.itatti.harvard.edu/resource/Start\n",
      "  Processing URL: https://cria.itatti.harvard.edu/\n",
      "  Processing URL: https://pharosartresearch.org/\n",
      "  Processing URL: https://yashiro.itatti.harvard.edu/\n",
      "  Processing URL: https://bellegreene.itatti.harvard.edu/resource/rsp:Letters\n",
      "Processing Digital humanties lab - Bibliotheca hertziana (rome)...\n",
      "  Processing URL: https://maps.biblhertz.it/gis/precat\n",
      "  Processing URL: https://biblhertz.github.io/atlas/\n",
      "  Processing URL: https://db.biblhertz.it/siena/siena.xql\n",
      "  Processing URL: https://wissensgeschichte.biblhertz.it/3d-bridge-html/index3D.html\n",
      "  Processing URL: https://dlib.biblhertz.it/\n",
      "  Processing URL: https://rara.biblhertz.it/\n",
      "  Processing URL: https://foto.biblhertz.it/\n",
      "  Processing URL: https://zuccaro.biblhertz.it/\n",
      "  Processing URL: https://dlib.biblhertz.it/urbs/#14.29/41.89547/12.46792/17.7\n",
      "  Processing URL: https://dlib.biblhertz.it/spatial/#15.96/41.895619/12.477807/20\n",
      "  Processing URL: https://dlib.biblhertz.it/guide/\n",
      "  Processing URL: https://db.biblhertz.it/noack/noack.xml\n",
      "  Processing URL: https://cipro.biblhertz.it/\n",
      "  Processing URL: https://fm.biblhertz.it/fmi/xsl/home.xsl?-token.proj=li\n",
      "  Processing URL: https://db.biblhertz.it/abruzzo/index.xml\n",
      "  Processing URL: https://wissensgeschichte.biblhertz.it/Glossario\n",
      "  Processing URL: https://maps.biblhertz.it/\n",
      "  Processing URL: https://staccioli.biblhertz.it/resource/Default:Start\n",
      "  Processing URL: https://echaurren.biblhertz.it/PE.html\n",
      "  Processing URL: https://dlib.biblhertz.it/places#17.35/41.896072/12.482376\n",
      "  Processing URL: https://data.biblhertz.it/\n",
      "  Processing URL: https://confluenze.dev/#16.5/42.348011/13.397988/-69/26\n",
      "  Processing URL: https://dlib.biblhertz.it/perspectiva/\n",
      "  Processing URL: https://editions.humanitiesconnect.pub/exist/apps/hwgw/index.html\n",
      "Processing DH Research Group – ICT Center – Fondazione Bruno Kessler ...\n",
      "  Processing URL: https://dh.fbk.eu/category/projects/\n",
      "Processing nan...\n",
      "Processing ILIESI Institute - cnr roma...\n",
      "  Processing URL: https://www.iliesi.cnr.it/attivita.php?tp=a_d\n",
      "Processing Institute for Computational Linguistics «A. Zampolli» - pisa...\n",
      "  Processing URL: https://www.ilc.cnr.it/progetti-attivi/\n",
      "  Processing URL: https://www.ilc.cnr.it/progetti/\n",
      "Processing Visual Computing Lab – ISTI – CNR -...\n",
      "  Processing URL: https://www.labcd.unipi.it/en/osservatorio/visual-computing-lab-isti-cnr-2/\n",
      "Found 312 projects across 18 institutions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def create_dh_projects_scraper():\n",
    "    def is_valid_url(url):\n",
    "        \"\"\"Check if the URL is valid, adding schema if missing.\"\"\"\n",
    "        if not url:\n",
    "            return None\n",
    "        url = url.strip()\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url\n",
    "\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return url if all([result.scheme, result.netloc]) else None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_page_content(url):\n",
    "        \"\"\"Fetch and parse webpage content with error handling.\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def find_project_links(soup, base_url):\n",
    "        \"\"\"Find project-related links on the page, including those in modals.\"\"\"\n",
    "        keywords = [\n",
    "            'project', 'progetti',\n",
    "            'digital humanities', 'umanistica digitale',\n",
    "            'digital', 'digitale',\n",
    "            'research', 'ricerca',\n",
    "            'laboratorio', 'laboratory',\n",
    "            'archivio', 'archive'\n",
    "        ]\n",
    "\n",
    "        exclude_domains = [\n",
    "            'facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com',\n",
    "            'youtube.com', 'blogspot.com', 'wordpress.com', 'tumblr.com'\n",
    "        ]\n",
    "\n",
    "        projects = []\n",
    "        if soup:\n",
    "            # Find all potential modal containers\n",
    "            modal_elements = soup.find_all([\n",
    "                'div',  # Common modal container\n",
    "                'section',  # Sometimes used for modals\n",
    "                'aside'  # Off-canvas modals\n",
    "            ], class_=lambda x: x and any(modal_term in x.lower()\n",
    "                for modal_term in ['modal', 'popup', 'dialog', 'overlay', 'lightbox']))\n",
    "\n",
    "            # Combine regular page content and modal content\n",
    "            search_areas = [soup] + modal_elements\n",
    "\n",
    "            for area in search_areas:\n",
    "                links = area.find_all('a', href=True)\n",
    "                for link in links:\n",
    "                    href = link.get('href', '')\n",
    "                    text = link.text.lower().strip()\n",
    "\n",
    "                    # Skip empty, javascript, email, or phone links\n",
    "                    if not href or href.startswith(('javascript:', '#', 'mailto:', 'tel:')):\n",
    "                        continue\n",
    "\n",
    "                    # Make URL absolute\n",
    "                    href = urljoin(base_url, href)\n",
    "\n",
    "                    # Check if the URL should be excluded\n",
    "                    if any(domain in href for domain in exclude_domains):\n",
    "                        continue\n",
    "\n",
    "                    # Check if any keyword is in the link text or URL\n",
    "                    if any(keyword in text or keyword in href.lower() for keyword in keywords):\n",
    "                        # Check for hidden elements\n",
    "                        is_hidden = any(parent.get('style', '').lower().find('display: none') != -1\n",
    "                                      or parent.get('style', '').lower().find('visibility: hidden') != -1\n",
    "                                      for parent in link.parents)\n",
    "\n",
    "                        if not is_hidden and not any(p['link'] == href for p in projects):\n",
    "                            projects.append({\n",
    "                                'link': href,\n",
    "                                'name': text or href,\n",
    "                            })\n",
    "\n",
    "        return projects\n",
    "\n",
    "    def process_institution(row):\n",
    "        \"\"\"Process a single institution's URLs.\"\"\"\n",
    "        projects_list = []\n",
    "        if pd.notna(row['URL']):\n",
    "            # Split URLs by semicolon and handle potential whitespace\n",
    "            urls = [url.strip() for url in str(row['URL']).split(';')]\n",
    "\n",
    "            # Check if this is one of the special institutions\n",
    "            special_institutions = ['Torino', 'Catania', 'Tatti', 'Hertziana']\n",
    "            is_special = any(inst in str(row['Nome']) for inst in special_institutions)\n",
    "\n",
    "            # For special institutions, just add the URLs as project links\n",
    "            if is_special:\n",
    "                for url in urls:\n",
    "                    valid_url = is_valid_url(url)\n",
    "                    if valid_url:\n",
    "                        projects_list.append({\n",
    "                            'categoria': row['Categoria'],\n",
    "                            'institution': row['Nome'],\n",
    "                            'location': row['Luogo'],\n",
    "                            'institution_url': valid_url,\n",
    "                            'project_name': f\"Project at {row['Nome']}\",\n",
    "                            'project_link': valid_url\n",
    "                        })\n",
    "            else:\n",
    "                # Normal processing for other institutions\n",
    "                for url in urls:\n",
    "                    valid_url = is_valid_url(url)\n",
    "                    if valid_url:\n",
    "                        print(f\"  Processing URL: {valid_url}\")\n",
    "                        soup = get_page_content(valid_url)\n",
    "                        if soup:\n",
    "                            projects = find_project_links(soup, valid_url)\n",
    "\n",
    "                            for project in projects:\n",
    "                                projects_list.append({\n",
    "                                    'categoria': row['Categoria'],\n",
    "                                    'institution': row['Nome'],\n",
    "                                    'location': row['Luogo'],\n",
    "                                    'institution_url': valid_url,\n",
    "                                    'project_name': project['name'],\n",
    "                                    'project_link': project['link']\n",
    "                                })\n",
    "\n",
    "        return projects_list\n",
    "\n",
    "    def process_dh_centers(csv_path):\n",
    "        \"\"\"Main function to process the CSV file and extract project links.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "\n",
    "            # Check if required columns are present\n",
    "            required_columns = ['Categoria', 'Nome', 'Luogo', 'URL']\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                raise ValueError(f\"CSV must contain columns: {', '.join(required_columns)}\")\n",
    "\n",
    "            # Process all institutions, including special ones\n",
    "            all_projects = []\n",
    "            for idx, row in df.iterrows():\n",
    "                print(f\"Processing {row['Nome']}...\")\n",
    "                projects = process_institution(row)\n",
    "                all_projects.extend(projects)\n",
    "\n",
    "            results_df = pd.DataFrame(all_projects)\n",
    "\n",
    "            # Remove duplicates\n",
    "            results_df = results_df.drop_duplicates(subset=['institution', 'project_link'])\n",
    "\n",
    "            return results_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing CSV: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    return process_dh_centers\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = create_dh_projects_scraper()\n",
    "    results = scraper('output.csv')\n",
    "\n",
    "    # Save results to CSV\n",
    "    if not results.empty:\n",
    "        results.to_csv('dh_projects_results.csv', index=False, encoding='utf-8')\n",
    "        print(f\"Found {len(results)} projects across {results['institution'].nunique()} institutions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMXL02YEMSax",
    "outputId": "e3919e74-d266-4960-d99b-9015ea76a062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 1/312: https://centri.unibo.it/dharc/en\n",
      "Checking 2/312: https://centri.unibo.it/dharc/en/research\n",
      "Checking 3/312: https://centri.unibo.it/dharc/en/research/topics\n",
      "Checking 4/312: https://centri.unibo.it/dharc/en/research/projects-at-dh-arc\n",
      "Checking 5/312: https://centri.unibo.it/dharc/en/research/partner\n",
      "Checking 6/312: https://centri.unibo.it/dharc/en/research/visiting-fellow\n",
      "Checking 7/312: http://www.iccd.beniculturali.it/it/progetti/4597/arco-architettura-della-conoscenza-ontologie-per-la-descrizione-del-patrimonio-culturale\n",
      "Checking 8/312: http://artchives.fondazionezeri.unibo.it/\n",
      "Checking 9/312: https://polifonia-project.github.io/clef/\n",
      "Checking 10/312: https://github.com/polifonia-project/registry_app\n",
      "Checking 11/312: https://projects.dharc.unibo.it/dhdkey/\n",
      "Checking 12/312: https://dl.ficlit.unibo.it\n",
      "Checking 13/312: https://aldomorodigitale.unibo.it/\n",
      "Checking 14/312: http://www.mario-project.eu/portal/\n",
      "Checking 15/312: https://www.iks-project.eu/\n",
      "Checking 16/312: https://icdp-digital-library.github.io/KNOT/\n",
      "Checking 17/312: https://projects.dharc.unibo.it/knot/\n",
      "Checking 18/312: https://projects.dharc.unibo.it/leggomanzoni/\n",
      "Checking 19/312: https://projects.dharc.unibo.it/lift/\n",
      "Checking 20/312: http://projects.dharc.unibo.it/mauth/search\n",
      "Checking 21/312: https://projects.dharc.unibo.it/melody/\n",
      "Checking 22/312: https://projects.dharc.unibo.it/odi/\n",
      "Checking 23/312: http://projects.dharc.unibo.it/philoeditor/\n",
      "Checking 24/312: https://raguproject.github.io/\n",
      "Checking 25/312: http://projects.dharc.unibo.it/bufalini-notebook/\n",
      "Checking 26/312: http://vespasianodabisticciletters.unibo.it/\n",
      "Checking 27/312: http://projects.dharc.unibo.it/vespasiano/\n",
      "Checking 28/312: https://shivadharmaproject.com/the-sivadhama-infrastructure-modeling-a-web-application-for-managing-scholarly-data/\n",
      "Checking 29/312: https://cordis.europa.eu/project/id/870811\n",
      "Checking 30/312: http://projects.dharc.unibo.it/uhdw/\n",
      "Checking 31/312: https://www.unive.it/pag/40315/\n",
      "Checking 32/312: https://pric.unive.it/projects/liber/home\n",
      "Checking 33/312: http://www.mqdq.it\n",
      "Checking 34/312: http://it/264/laboratorio-di-epigrafia-greca\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    954\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\http\\client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1283\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\http\\client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\http\\client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\http\\client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1041\u001b[0m \n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\http\\client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x00000180C3958A00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 119\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 119\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_links_status\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdh_projects_results.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;66;03m# Display a sample of the results\u001b[39;00m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample of results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m, in \u001b[0;36mcheck_links_status\u001b[1;34m(csv_path)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Determine website status\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connectionpool.py:830\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[0;32m    827\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    829\u001b[0m     )\n\u001b[1;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    831\u001b[0m         method,\n\u001b[0;32m    832\u001b[0m         url,\n\u001b[0;32m    833\u001b[0m         body,\n\u001b[0;32m    834\u001b[0m         headers,\n\u001b[0;32m    835\u001b[0m         retries,\n\u001b[0;32m    836\u001b[0m         redirect,\n\u001b[0;32m    837\u001b[0m         assert_same_host,\n\u001b[0;32m    838\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    839\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    840\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    841\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    842\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    843\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[0;32m    844\u001b[0m     )\n\u001b[0;32m    846\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[0;32m    847\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connectionpool.py:805\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    800\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m    802\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    803\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    804\u001b[0m )\n\u001b[1;32m--> 805\u001b[0m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m err \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\retry.py:434\u001b[0m, in \u001b[0;36mRetry.sleep\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m slept:\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sleep_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\retry.py:418\u001b[0m, in \u001b[0;36mRetry._sleep_backoff\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backoff \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 418\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "def create_session_with_retries():\n",
    "    \"\"\"Create a requests session with retry strategy\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def check_links_status(csv_path):\n",
    "    \"\"\"Check status of project links and create a clean status report\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # Create a session with retry strategy\n",
    "    session = create_session_with_retries()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Create lists to store results\n",
    "    results = []\n",
    "\n",
    "    # Check each link\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['project_link']\n",
    "        print(f\"Checking {index + 1}/{len(df)}: {url}\")\n",
    "\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, timeout=10, allow_redirects=True)\n",
    "            status_code = response.status_code\n",
    "\n",
    "            # Determine website status\n",
    "            if 200 <= status_code < 300:\n",
    "                status = \"Active\"\n",
    "            elif status_code == 404:\n",
    "                status = \"Not Found\"\n",
    "            elif 300 <= status_code < 400:\n",
    "                status = f\"Redirect ({status_code})\"\n",
    "            elif 400 <= status_code < 500:\n",
    "                status = f\"Client Error ({status_code})\"\n",
    "            elif 500 <= status_code < 600:\n",
    "                status = f\"Server Error ({status_code})\"\n",
    "            else:\n",
    "                status = f\"Unknown ({status_code})\"\n",
    "\n",
    "            results.append({\n",
    "                'institution': row['institution'],\n",
    "                'project_name': row['project_name'],\n",
    "                'url': url,\n",
    "                'status_code': status_code,\n",
    "                'status': status,\n",
    "                'response': \"Success\" if 200 <= status_code < 300 else \"Failed\"\n",
    "            })\n",
    "\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            results.append({\n",
    "                'institution': row['institution'],\n",
    "                'project_name': row['project_name'],\n",
    "                'url': url,\n",
    "                'status_code': None,\n",
    "                'status': \"Connection Error\",\n",
    "                'response': \"Failed\"\n",
    "            })\n",
    "        except requests.exceptions.Timeout:\n",
    "            results.append({\n",
    "                'institution': row['institution'],\n",
    "                'project_name': row['project_name'],\n",
    "                'url': url,\n",
    "                'status_code': None,\n",
    "                'status': \"Timeout\",\n",
    "                'response': \"Failed\"\n",
    "            })\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            results.append({\n",
    "                'institution': row['institution'],\n",
    "                'project_name': row['project_name'],\n",
    "                'url': url,\n",
    "                'status_code': None,\n",
    "                'status': f\"Error: {str(e)[:100]}...\",\n",
    "                'response': \"Failed\"\n",
    "            })\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Create DataFrame from results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_links = len(results_df)\n",
    "    active_links = len(results_df[results_df['response'] == \"Success\"])\n",
    "    failed_links = total_links - active_links\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total links checked: {total_links}\")\n",
    "    print(f\"Active links: {active_links}\")\n",
    "    print(f\"Failed links: {failed_links}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df.to_csv('website_status_report.csv', index=False, encoding='utf-8')\n",
    "    print(\"\\nDetailed results saved to: website_status_report.csv\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df = check_links_status('dh_projects_results.csv')\n",
    "\n",
    "    if results_df is not None:\n",
    "        # Display a sample of the results\n",
    "        print(\"\\nSample of results:\")\n",
    "        print(results_df[['institution', 'project_name', 'url', 'status', 'response']].head())\n",
    "\n",
    "        # Display breakdown of different status types\n",
    "        print(\"\\nStatus breakdown:\")\n",
    "        print(results_df['status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UONji9rYMaYL",
    "outputId": "dea94c16-7c2d-40fe-b77d-6710e492b390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 13 non-200 status links...\n",
      "Error checking http://artchives.fondazionezeri.unibo.it/: HTTPConnectionPool(host='web.archive.org', port=80): Read timed out. (read timeout=10)\n",
      "Processing 15 of 13 URLs...\n",
      "Error checking https://www.iks-project.eu/: HTTPConnectionPool(host='web.archive.org', port=80): Read timed out. (read timeout=10)\n",
      "Processing 20 of 13 URLs...\n",
      "Error checking http://projects.dharc.unibo.it/mauth/search: HTTPConnectionPool(host='web.archive.org', port=80): Read timed out. (read timeout=10)\n",
      "Error checking http://it/264/laboratorio-di-epigrafia-greca: HTTPConnectionPool(host='web.archive.org', port=80): Read timed out. (read timeout=10)\n",
      "Error checking http://vbd.humnet.unipi.it/beta2/: HTTPConnectionPool(host='web.archive.org', port=80): Read timed out. (read timeout=10)\n",
      "Error checking https://italianacademy.columbia.edu/content/frida: HTTPConnectionPool(host='web.archive.org', port=80): Read timed out. (read timeout=10)\n",
      "Error checking https://web.unica.it/unica/it/utility_ricerca.page: HTTPConnectionPool(host='web.archive.org', port=80): Read timed out. (read timeout=10)\n",
      "Processing 235 of 13 URLs...\n",
      "Error checking https://www.comicon.it/class/alla-ricerca-dellunderground-riviste-stampa-alternativa-generazioni/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /cdx/search/cdx?url=https://www.comicon.it/class/alla-ricerca-dellunderground-riviste-stampa-alternativa-generazioni/&output=json&filter=statuscode:200 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d73430ce1a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "Results saved to website_status_with_wayback.csv\n",
      "\n",
      "Summary:\n",
      "Total non-200 links processed: 13\n",
      "Links found in Wayback Machine: 5\n",
      "Links not found: 8\n",
      "\n",
      "Archived links details:\n",
      "URL: http://www.iccd.beniculturali.it/it/progetti/4597/arco-architettura-della-conoscenza-ontologie-per-la-descrizione-del-patrimonio-culturale\n",
      "Original Status: nan\n",
      "Last Accessible: 2024-06-24\n",
      "--------------------------------------------------\n",
      "URL: http://vcg.isti.cnr.it/cross/\n",
      "Original Status: 404.0\n",
      "Last Accessible: 2023-11-29\n",
      "--------------------------------------------------\n",
      "URL: https://sourceforge.net/projects/evt-project/\n",
      "Original Status: 403.0\n",
      "Last Accessible: 2023-10-06\n",
      "--------------------------------------------------\n",
      "URL: http://evt.labcd.unipi.it/\n",
      "Original Status: nan\n",
      "Last Accessible: 2024-09-10\n",
      "--------------------------------------------------\n",
      "URL: http://www.getty.edu/research/\n",
      "Original Status: nan\n",
      "Last Accessible: 2024-10-08\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_last_accessible_date(url):\n",
    "    \"\"\"\n",
    "    Uses the Wayback Machine CDX API to find the most recent snapshot date when the URL was accessible.\n",
    "    \"\"\"\n",
    "    if not url or pd.isna(url):  # Check for empty or NaN values\n",
    "        return None\n",
    "\n",
    "    api_url = f\"http://web.archive.org/cdx/search/cdx?url={url}&output=json&filter=statuscode:200\"\n",
    "    try:\n",
    "        response = requests.get(api_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Skip header and find the most recent snapshot with a 200 status code\n",
    "        if len(data) > 1:\n",
    "            last_snapshot = data[-1]  # Last entry\n",
    "            timestamp = last_snapshot[1]  # CDX API timestamp format is YYYYMMDDhhmmss\n",
    "            # Convert timestamp to a human-readable date\n",
    "            last_accessible_date = datetime.strptime(timestamp, \"%Y%m%d%H%M%S\").date()\n",
    "            return last_accessible_date\n",
    "        else:\n",
    "            return None  # No accessible snapshot found\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error checking {url}: {str(e)}\")\n",
    "        return None  # Return None instead of error message for consistency\n",
    "\n",
    "def check_non200_links_in_wayback(status_report_csv):\n",
    "    \"\"\"\n",
    "    Process only non-200 status links from the status report and check their last accessible date in Wayback Machine.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the status report CSV file\n",
    "        df = pd.read_csv(status_report_csv)\n",
    "\n",
    "        required_columns = ['url', 'status_code']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV file must contain columns: {', '.join(required_columns)}\")\n",
    "\n",
    "        # Filter for non-200 status codes\n",
    "        non200_df = df[df['status_code'] != 200].copy()\n",
    "\n",
    "        if len(non200_df) == 0:\n",
    "            print(\"No non-200 status links found to process.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Processing {len(non200_df)} non-200 status links...\")\n",
    "\n",
    "        # New column to store the last accessible date from Wayback Machine\n",
    "        non200_df['wayback_last_accessible'] = None\n",
    "\n",
    "        # Add a counter for progress tracking\n",
    "        total = len(non200_df)\n",
    "        for index, row in non200_df.iterrows():\n",
    "            if (index + 1) % 5 == 0:  # Show progress every 5 items\n",
    "                print(f\"Processing {index + 1} of {total} URLs...\")\n",
    "\n",
    "            url = row['url']\n",
    "            last_accessible_date = get_last_accessible_date(url)\n",
    "            non200_df.at[index, 'wayback_last_accessible'] = last_accessible_date\n",
    "\n",
    "            # Add a small delay to avoid overwhelming the API\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Merge results back with original DataFrame\n",
    "        df = df.merge(\n",
    "            non200_df[['url', 'wayback_last_accessible']],\n",
    "            on='url',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Save results to a new CSV file\n",
    "        output_file = 'website_status_with_wayback.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nResults saved to {output_file}\")\n",
    "\n",
    "        # Print summary statistics\n",
    "        total_non200 = len(non200_df)\n",
    "        archived_links = non200_df['wayback_last_accessible'].notna().sum()\n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"Total non-200 links processed: {total_non200}\")\n",
    "        print(f\"Links found in Wayback Machine: {archived_links}\")\n",
    "        print(f\"Links not found: {total_non200 - archived_links}\")\n",
    "\n",
    "        # Display results for archived links\n",
    "        if archived_links > 0:\n",
    "            print(\"\\nArchived links details:\")\n",
    "            archived_df = non200_df[non200_df['wayback_last_accessible'].notna()]\n",
    "            for _, row in archived_df.iterrows():\n",
    "                print(f\"URL: {row['url']}\")\n",
    "                print(f\"Original Status: {row['status_code']}\")\n",
    "                print(f\"Last Accessible: {row['wayback_last_accessible']}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Usage example\n",
    "    results = check_non200_links_in_wayback('website_status_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved as filtered_DH_institutions_with_digital_editions.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'filtering_DH_institutions_from_links.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Set column names correctly and remove header row if needed\n",
    "df.columns = df.iloc[0]\n",
    "df = df[1:].reset_index(drop=True)\n",
    "df.columns = [\"category\", \"institution\", \"location\", \"institution_url\", \"project_name\", \"project_link\"]\n",
    "\n",
    "# Add a new column for Digital Edition detection\n",
    "df[\"Digital Edition\"] = \"\"\n",
    "\n",
    "# Define the function to check for digital edition keywords\n",
    "def check_digital_edition(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            page_content = response.text.lower()\n",
    "            # Look for keywords related to digital editions\n",
    "            if \"edizioni digitali\" in page_content or \"digital edition\" in page_content or \"edizione digitale\" in page_content:\n",
    "                return \"yes\"\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "    return \"\"  # Leave blank if no keywords found or if there's an error\n",
    "\n",
    "# Apply the function to each URL\n",
    "df[\"Digital Edition\"] = df[\"project_link\"].apply(check_digital_edition)\n",
    "\n",
    "# Save the updated DataFrame back to a new Excel file\n",
    "output_path = 'filtered_DH_institutions_with_digital_editions.xlsx'\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Processed file saved as {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
